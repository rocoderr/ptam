<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>3. Requiring platforms to proactively remove and ban “high-risk content” · Political Triple-Axis Model</title>
    <link rel="stylesheet" href="../assets/site.css" />
  </head>
  <body data-base="../">
    <div class="app">
      <aside class="sidebar">
        <div class="sidebar__top">
          <a class="brand" href="../index.html">Political Triple-Axis Model</a>
          <div class="lang">
            <a href="../zh/index.html">中文</a>
            <a href="../en/index.html">English</a>
            <a href="../axis/index.html">3D</a>
          </div>
        </div>
        <nav id="toc" class="toc" data-lang="en"></nav>
      </aside>

      <main class="main">
        <article class="content" id="content"></article>
        <script type="text/markdown" id="md-source"><a id="sec-9-3"></a>
### 3. Requiring platforms to proactively remove and ban “high-risk content”

#### 3.1 Initial Position: platforms as neutral intermediaries with limited liability

In early digital-platform governance, platforms were often treated as **content intermediaries** rather than content judges:

- platforms did not bear substantive responsibility for user expression;  
- governance was mainly ex post and case-based;  
- user expression space was not filtered ex ante by default.

In three-axis terms, this default implies:

- **Y:** expression restriction is mainly ex post; coercion boundary is relatively rear-guard (Y+).  
- **Z:** speech liberty is treated as a pre-political right; platform rules are treated as technical/contractual arrangements (Z+).  
- **X:** platforms compete as market actors; moderation cost is treated as business cost, not public obligation.

#### 3.2 Contextual Scenario: a platform removes “high-risk content”

Consider:

- a large social platform hosts a short video containing (1) hostile description of a specific group and (2) suggestive mobilization toward real-world action (time/place plus “go do something”);  
- the platform removes it and bans the account; debate follows:
  - supporters: necessary to prevent real harm;  
  - opponents: a substantive violation of speech liberty and digital rights.

The key is not “offensiveness”. It is whether platforms should be required to intervene **before risk materializes**.

#### 3.3 Motion Statement

**When user content is judged likely to cause real-world harm or public-order breakdown, platforms should proactively govern such content through ex ante review, takedown, and banning to prevent risk.**

The core is shifting platform responsibility position from post-hoc correction to pre-risk intervention.

#### 3.4 Structural Consequences: governance power concentrates; expression space shrinks ex ante

- **Y:** restriction shifts from ex post correction to ex ante filtering; workable expression space is front-loadedly compressed (Y→−).  
- **Z:** speech is no longer treated as a right immune to ex ante intervention; it slides into platform-permission and risk-evaluation structures (Z→−).  
- **X:** proactive-governance capacity and compliance cost concentrate in large platforms, creating structural competitive advantages and squeezing out small entrants.

Platforms shift from intermediaries toward **quasi-governance nodes**, de facto carrying parts of public-order maintenance functions.
</script>
      </main>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/markdown-it@13.0.1/dist/markdown-it.min.js"></script>
    <script src="../assets/markdown-render.js"></script>
    <script src="../assets/toc.js"></script>
  </body>
</html>